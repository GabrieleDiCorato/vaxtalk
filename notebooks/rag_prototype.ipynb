{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RAG Vaccine Information System\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system for vaccine information using Google's Gemini AI. The system:\n",
    "- Ingests documents from PDFs and websites\n",
    "- Creates a searchable vector database using embeddings\n",
    "- Provides an AI agent that answers questions using retrieved context\n",
    "\n",
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required libraries and configure API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.tools import FunctionTool\n",
    "from google.genai import types\n",
    "from google.adk.apps.app import App\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "\n",
    "print(\"✅ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Project Path Configuration\n",
    "\n",
    "Add the project root to Python's import path to access custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the project root (the parent of the \"src\" directory) is on sys.path\n",
    "# so that \"import src.model\" finds the src package under the project root.\n",
    "project_root = Path.cwd().parent\n",
    "src_dir = project_root / \"src\"\n",
    "\n",
    "project_root_path = str(project_root.resolve())\n",
    "if project_root_path not in sys.path:\n",
    "    sys.path.insert(0, project_root_path)\n",
    "\n",
    "from src.model import Intensity, SentimentOutput\n",
    "from src.model.rag_output import RagOutput\n",
    "from src.config import load_env_variables, get_env_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_env_variables()\n",
    "\n",
    "GOOGLE_API_KEY = get_env_variable(\"GOOGLE_API_KEY\")\n",
    "print(f\"✅ API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### API Configuration\n",
    "\n",
    "Load environment variables and configure retry policy for API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=3,\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Data Structures\n",
    "\n",
    "Define the core data structures used throughout the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import heapq\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from pypdf import PdfReader\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path as FilePath\n",
    "from google import genai\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Represents a chunk of text from a document with metadata.\"\"\"\n",
    "    id: int\n",
    "    content: str\n",
    "    source: str      # URL or file path\n",
    "    doc_type: str    # \"web\" or \"pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Document Processing\n",
    "\n",
    "Functions to load, parse, and chunk documents from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str,\n",
    "               source: str,\n",
    "               doc_type: str,\n",
    "               chunk_size: int = 800,\n",
    "               chunk_overlap: int = 200,\n",
    "               start_id: int = 0) -> list[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks for RAG.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to chunk\n",
    "        source: Source identifier (URL or file path)\n",
    "        doc_type: Type of document (\"web\" or \"pdf\")\n",
    "        chunk_size: Number of words per chunk\n",
    "        chunk_overlap: Number of overlapping words between chunks\n",
    "        start_id: Starting ID for chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of DocumentChunk objects\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    current_id = start_id\n",
    "\n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append(\n",
    "                DocumentChunk(\n",
    "                    id=current_id,\n",
    "                    content=chunk_text,\n",
    "                    source=source,\n",
    "                    doc_type=doc_type\n",
    "                )\n",
    "            )\n",
    "            current_id += 1\n",
    "        i += chunk_size - chunk_overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### PDF Processing\n",
    "\n",
    "Load and chunk PDF documents from a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_folder(folder_path: str) -> list[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Reads all PDFs in a folder and converts them to chunks.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing PDF files\n",
    "        \n",
    "    Returns:\n",
    "        List of DocumentChunk objects from all PDFs\n",
    "    \"\"\"\n",
    "    all_chunks: list[DocumentChunk] = []\n",
    "    current_id = 0\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"[PDF] Folder not found: {folder_path}\")\n",
    "        return []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        print(f\"[PDF] Loading: {full_path}\")\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(full_path)\n",
    "            text_pages = []\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text_pages.append(page_text)\n",
    "\n",
    "            full_text = \"\\n\".join(text_pages)\n",
    "            chunks = chunk_text(\n",
    "                text=full_text,\n",
    "                source=full_path,\n",
    "                doc_type=\"pdf\",\n",
    "                start_id=current_id\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "            current_id = all_chunks[-1].id + 1 if all_chunks else current_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[PDF] ERROR on {full_path}: {e}\")\n",
    "\n",
    "    print(f\"[PDF] Total chunks from PDFs: {len(all_chunks)}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "\n",
    "Crawl websites and extract text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_domain(url: str, root_netloc: str) -> bool:\n",
    "    \"\"\"Check if a URL belongs to the same domain.\"\"\"\n",
    "    try:\n",
    "        return urlparse(url).netloc == root_netloc\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_text_from_html(html: str) -> str:\n",
    "    \"\"\"Extract clean text from HTML, removing scripts and styles.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    # Normalize whitespace\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def crawl_website(root_url: str,\n",
    "                  max_pages: int = 100,\n",
    "                  max_depth: int = 3,\n",
    "                  start_id: int = 0) -> list[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Crawl a website and extract text chunks from all pages.\n",
    "    \n",
    "    Uses BFS to follow internal links up to a maximum depth.\n",
    "    Only crawls pages from the same domain as the root URL.\n",
    "    \n",
    "    Args:\n",
    "        root_url: Starting URL for the crawl\n",
    "        max_pages: Maximum number of pages to visit\n",
    "        max_depth: Maximum link depth from root\n",
    "        start_id: Starting ID for chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of DocumentChunk objects from all crawled pages\n",
    "    \"\"\"\n",
    "    parsed_root = urlparse(root_url)\n",
    "    root_netloc = parsed_root.netloc\n",
    "\n",
    "    visited = set()\n",
    "    to_visit: list[tuple[str, int]] = [(root_url, 0)]\n",
    "    all_chunks: list[DocumentChunk] = []\n",
    "    current_id = start_id\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": \"rag-bot/1.0\"})\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url, depth = to_visit.pop(0)\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        if depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"[WEB] Downloading ({depth}): {url}\")\n",
    "            resp = session.get(url, timeout=10)\n",
    "            if \"text/html\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                continue\n",
    "\n",
    "            text = extract_text_from_html(resp.text)\n",
    "            if text.strip():\n",
    "                chunks = chunk_text(\n",
    "                    text=text,\n",
    "                    source=url,\n",
    "                    doc_type=\"web\",\n",
    "                    start_id=current_id\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "                current_id = all_chunks[-1].id + 1 if all_chunks else current_id\n",
    "\n",
    "            # Extract links and add to queue\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                full_url = urljoin(url, href)\n",
    "                parsed = urlparse(full_url)\n",
    "\n",
    "                # Keep only HTTP/HTTPS links from same domain\n",
    "                if parsed.scheme not in (\"http\", \"https\"):\n",
    "                    continue\n",
    "                if not is_same_domain(full_url, root_netloc):\n",
    "                    continue\n",
    "                if full_url not in visited:\n",
    "                    to_visit.append((full_url, depth + 1))\n",
    "\n",
    "            # Polite crawling delay\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WEB] ERROR on {url}: {e}\")\n",
    "\n",
    "    print(f\"[WEB] Total pages visited: {len(visited)}\")\n",
    "    print(f\"[WEB] Total chunks from site: {len(all_chunks)}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 4. Embedding and Retrieval\n",
    "\n",
    "Functions for creating embeddings and retrieving similar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini Client for embeddings\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Cache configuration\n",
    "CACHE_DIR = FilePath(\"../cache\")\n",
    "EMBEDDINGS_CACHE_FILE = CACHE_DIR / \"embeddings.pkl\"\n",
    "CHUNKS_CACHE_FILE = CACHE_DIR / \"chunks.pkl\"\n",
    "\n",
    "def save_index_to_cache(embeddings: np.ndarray, chunks: list[DocumentChunk]):\n",
    "    \"\"\"\n",
    "    Save embeddings and chunks to disk cache for faster loading.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Numpy array of embedding vectors\n",
    "        chunks: List of DocumentChunk objects\n",
    "    \"\"\"\n",
    "    CACHE_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(EMBEDDINGS_CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    \n",
    "    with open(CHUNKS_CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    \n",
    "    print(f\"✅ Index cached to {CACHE_DIR}\")\n",
    "\n",
    "def load_index_from_cache() -> tuple[np.ndarray, list[DocumentChunk]]:\n",
    "    \"\"\"\n",
    "    Load embeddings and chunks from disk cache.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, chunks list) or (empty array, empty list) if not cached\n",
    "    \"\"\"\n",
    "    if not EMBEDDINGS_CACHE_FILE.exists() or not CHUNKS_CACHE_FILE.exists():\n",
    "        return np.array([]), []\n",
    "    \n",
    "    try:\n",
    "        with open(EMBEDDINGS_CACHE_FILE, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        \n",
    "        with open(CHUNKS_CACHE_FILE, 'rb') as f:\n",
    "            chunks = pickle.load(f)\n",
    "        \n",
    "        print(f\"✅ Index loaded from cache: {len(chunks)} chunks\")\n",
    "        return embeddings, chunks\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading cache: {e}\")\n",
    "        return np.array([]), []\n",
    "\n",
    "def embed_texts(texts: list[str],\n",
    "                batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Google Gemini API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        batch_size: Number of texts to embed per API call\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of shape (n, d) where n is number of texts and d is embedding dimension\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "\n",
    "    all_vectors = []\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            print(f\"[EMB] Batch {i}–{i+len(batch)-1} of {len(texts)}\")\n",
    "\n",
    "            # Gemini embedding call\n",
    "            response = client.models.embed_content(\n",
    "                model=\"text-embedding-004\",\n",
    "                contents=batch\n",
    "            )\n",
    "            # Extract embedding vectors from response\n",
    "            vectors = [item.values for item in response.embeddings]\n",
    "            all_vectors.extend(vectors)\n",
    "\n",
    "        return np.array(all_vectors, dtype=np.float32)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def build_vector_index(chunks: list[DocumentChunk]) -> tuple[np.ndarray, list[DocumentChunk]]:\n",
    "    \"\"\"\n",
    "    Create embedding vectors for all document chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of DocumentChunk objects\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, chunks list)\n",
    "    \"\"\"\n",
    "    print(\"[INDEX] Calculating embeddings for all chunks...\")\n",
    "    texts = [c.content for c in chunks]\n",
    "    embeddings = embed_texts(texts)\n",
    "    print(f\"[INDEX] Embeddings shape: {embeddings.shape}\")\n",
    "    return embeddings, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between multiple vectors and a single query vector.\n",
    "    \n",
    "    Args:\n",
    "        a: Matrix of shape (n, d) containing n vectors of dimension d\n",
    "        b: Single vector of shape (d,)\n",
    "        \n",
    "    Returns:\n",
    "        Array of shape (n,) with similarity scores\n",
    "    \"\"\"\n",
    "    if a.size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-10)\n",
    "    b_norm = b / (np.linalg.norm(b) + 1e-10)\n",
    "    return np.dot(a_norm, b_norm)\n",
    "\n",
    "\n",
    "def retrieve_top_k(query: str,\n",
    "                   embeddings: np.ndarray,\n",
    "                   chunks: list[DocumentChunk],\n",
    "                   k: int = 5) -> list[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar chunks to a query.\n",
    "    \n",
    "    Embeds the query and finds the k chunks with highest cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Query string\n",
    "        embeddings: Matrix of chunk embeddings (n, d)\n",
    "        chunks: List of DocumentChunk objects corresponding to embeddings\n",
    "        k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of top-k DocumentChunk objects, ordered by similarity (highest first)\n",
    "    \"\"\"\n",
    "    if embeddings.size == 0 or not chunks:\n",
    "        return []\n",
    "\n",
    "    query_emb = embed_texts([query])[0]  # (d,)\n",
    "    sims = cosine_similarity_matrix(embeddings, query_emb)  # (n,)\n",
    "\n",
    "    # Get indices of top k similarities using heapq\n",
    "    k = min(k, len(chunks))\n",
    "    top_k_items = heapq.nlargest(k, enumerate(sims), key=lambda x: x[1])\n",
    "    \n",
    "    # Extract chunks in order of similarity (highest first)\n",
    "    top_chunks = [chunks[idx] for idx, _ in top_k_items]\n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 5. Knowledge Base Initialization\n",
    "\n",
    "Build or load the vector index from document sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"pdf_folder\": str((project_root / \"src\" / \"Doc_vaccini\").resolve()),\n",
    "    \"root_url\": \"https://www.serviziterritoriali-asstmilano.it/servizi/vaccinazioni/\",\n",
    "    \"max_pages\": 10,\n",
    "    \"max_depth\": 2,\n",
    "    \"use_cache\": True\n",
    "}\n",
    "\n",
    "# Initialize global index\n",
    "global_embeddings = np.array([])\n",
    "global_chunks = []\n",
    "\n",
    "# Load from cache or build fresh\n",
    "if CONFIG[\"use_cache\"]:\n",
    "    global_embeddings, global_chunks = load_index_from_cache()\n",
    "\n",
    "if global_embeddings.size == 0:\n",
    "    print(\"Building knowledge base...\")\n",
    "    \n",
    "    # Load and process documents\n",
    "    pdf_chunks = load_pdfs_from_folder(CONFIG[\"pdf_folder\"])\n",
    "    web_start_id = pdf_chunks[-1].id + 1 if pdf_chunks else 0\n",
    "    web_chunks = crawl_website(\n",
    "        CONFIG[\"root_url\"], \n",
    "        max_pages=CONFIG[\"max_pages\"], \n",
    "        max_depth=CONFIG[\"max_depth\"], \n",
    "        start_id=web_start_id\n",
    "    )\n",
    "    \n",
    "    # Build index\n",
    "    all_chunks = pdf_chunks + web_chunks\n",
    "    if all_chunks:\n",
    "        global_embeddings, global_chunks = build_vector_index(all_chunks)\n",
    "        save_index_to_cache(global_embeddings, global_chunks)\n",
    "        print(f\"✅ Index built: {len(global_chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"⚠️ No content found\")\n",
    "else:\n",
    "    print(f\"✅ Loaded from cache: {len(global_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_vaccine_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves vaccine information from the knowledge base.\n",
    "    \n",
    "    This function is exposed to the AI agent as a tool.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question about vaccines\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with relevant information and source citations\n",
    "    \"\"\"\n",
    "    if not global_chunks:\n",
    "        return \"Error: Knowledge base not initialized. Run ingestion cell first.\"\n",
    "    \n",
    "    try:\n",
    "        top_chunks = retrieve_top_k(query, global_embeddings, global_chunks, k=5)\n",
    "        if not top_chunks:\n",
    "            return \"No relevant information found.\"\n",
    "        \n",
    "        # Format results with source citations\n",
    "        results = []\n",
    "        for c in top_chunks:\n",
    "            source = FilePath(c.source).name if c.doc_type == \"pdf\" else c.source\n",
    "            results.append(f\"[SOURCE: {source}]\\n{c.content}\")\n",
    "        \n",
    "        return \"\\n\\n---\\n\\n\".join(results)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Create tool wrapper for the agent\n",
    "rag_tool = FunctionTool(retrieve_vaccine_info)\n",
    "print(f\"✅ Tool ready\" if global_chunks else \"⚠️ Run ingestion first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 6. RAG Tool\n",
    "\n",
    "Create the retrieval function that the agent will use to access the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 7. Agent Configuration\n",
    "\n",
    "Configure the RAG agent with the retrieval tool and structured output schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a helpful assistant for vaccine information.\n",
    "You have access to a knowledge base containing official documents and web pages about vaccinations.\n",
    "    \n",
    "When the user asks a question:\n",
    "1. Use the `retrieve_vaccine_info` tool to find relevant information.\n",
    "2. Answer the question based ONLY on the information returned by the tool.\n",
    "3. If the tool returns no information, or the information is not pertinent, return an error in the format specified below\n",
    "4. Always cite the sources provided in the tool output.\n",
    "5. Be concise but thorough in your responses.\n",
    "\"\"\"\n",
    "\n",
    "rag_agent = Agent(\n",
    "    name=\"RAG_Vaccine_Informer\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-flash-lite\", \n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=prompt,\n",
    "    tools=[rag_tool],\n",
    "    output_key=\"rag_output\",\n",
    "    #output_schema=RagOutput,\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Agent configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 8. Application Setup\n",
    "\n",
    "Create the application and runner instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session service for managing conversation state\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Create application with RAG agent as root\n",
    "application = App(\n",
    "    name=\"VaccineInfoRAG\",\n",
    "    root_agent=rag_agent\n",
    ")\n",
    "\n",
    "# Create runner to execute queries\n",
    "runner = Runner(\n",
    "    app=application, \n",
    "    session_service=session_service\n",
    ")\n",
    "\n",
    "print(\"✅ Application ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 9. Testing\n",
    "\n",
    "Test the RAG system with sample queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Single Query Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await runner.run_debug(\"Tell me the policy for pregnant woment vaccination\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaxtalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
