{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sentiment Function Tool Validation\n",
    "This notebook reuses the production `SentimentService` and mirrors the ADK `FunctionTool` logic so we can verify sentiment outcomes offline before wiring them into the multi-agent workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Configure Environment Paths\n",
    "Set the working directory to the project root so imports behave the same way they do inside the packaged app, and double-check that the `docs/` and `cache/` folders referenced in the runtime logs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "DOCS_DIR = PROJECT_ROOT / 'docs'\n",
    "CACHE_DIR = PROJECT_ROOT / 'cache'\n",
    "print(f\"Project root set to: {PROJECT_ROOT}\")\n",
    "print(f\"Docs directory exists: {DOCS_DIR.exists()} -> {DOCS_DIR}\")\n",
    "print(f\"Cache directory exists: {CACHE_DIR.exists()} -> {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Import Sentiment Tooling\n",
    "Load environment variables, configure logging helpers, and ensure the sentiment models used in production (`google/gemini-embedding-001` and `mistralai/ministral-8b`) are available for later checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from vaxtalk.config.logging_config import setup_logging\n",
    "from vaxtalk.model import SentimentOutput, Intensity\n",
    "from vaxtalk.sentiment.sentiment_service import SentimentService\n",
    "\n",
    "logger = setup_logging(log_dir=PROJECT_ROOT / 'logs', log_level='INFO')\n",
    "env_loaded = load_dotenv(PROJECT_ROOT / '.env')\n",
    "print(f\".env loaded: {env_loaded}\")\n",
    "\n",
    "required_envs = [\n",
    "    'OPENROUTER_API_KEY',\n",
    "    'OPENROUTER_EMBED_URL',\n",
    "    'OPENROUTER_CHAT_URL',\n",
    "]\n",
    "missing_envs = [var for var in required_envs if not os.getenv(var)]\n",
    "if missing_envs:\n",
    "    raise RuntimeError(f\"Missing required environment variables: {missing_envs}\")\n",
    "\n",
    "EMBEDDING_MODEL_ID = 'google/gemini-embedding-001'\n",
    "LLM_MODEL_ID = 'mistralai/ministral-8b'\n",
    "print(f\"Embedding model identifier ready: {EMBEDDING_MODEL_ID}\")\n",
    "print(f\"Sentiment LLM identifier ready: {LLM_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Instantiate SentimentService\n",
    "Create the hybrid sentiment analyzer with the production fusion weights, load `sentiment_phrases.json`, and guard that all 15 cached embeddings are available before running tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "sentiment_service = SentimentService()\n",
    "sentiment_service.build_sentiment_phrases_embeddings(use_cache=True)\n",
    "proto_stats = sentiment_service.get_stats()\n",
    "print(\n",
    "    \"Sentiment prototypes loaded:\",\n",
    "    proto_stats.get('total'),\n",
    "    proto_stats.get('by_emotion')\n",
    ")\n",
    "print(\n",
    "    f\"Fusion weights -> LLM: {sentiment_service.w_llm:.2f}, \"\n",
    "    f\"Embeddings: {sentiment_service.w_emb:.2f}\"\n",
    ")\n",
    "\n",
    "if proto_stats.get('total', 0) < 15:\n",
    "    raise RuntimeError(\n",
    "        \"Expected at least 15 sentiment prototypes; rebuild cache with load-corpus.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _neutral_sentiment_output() -> SentimentOutput:\n",
    "    return SentimentOutput(\n",
    "        satisfaction=Intensity.LOW,\n",
    "        frustration=Intensity.LOW,\n",
    "        confusion=Intensity.LOW,\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NotebookToolContext:\n",
    "    state: dict[str, Any]\n",
    "\n",
    "\n",
    "def run_sentiment_tool(tool_context: NotebookToolContext) -> dict[str, Any]:\n",
    "    user_input = str(tool_context.state.get('user:input', '')).strip()\n",
    "\n",
    "    if not user_input:\n",
    "        result = _neutral_sentiment_output()\n",
    "        reason = 'empty_input'\n",
    "    else:\n",
    "        try:\n",
    "            result = sentiment_service.analyze_emotion(user_input)\n",
    "            reason = 'ok'\n",
    "        except RuntimeError:\n",
    "            sentiment_service.build_sentiment_phrases_embeddings(use_cache=False)\n",
    "            result = sentiment_service.analyze_emotion(user_input)\n",
    "            reason = 'rebuilt'\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            logger.error(\"Sentiment tool fallback triggered: %s\", exc)\n",
    "            result = _neutral_sentiment_output()\n",
    "            reason = 'fallback'\n",
    "\n",
    "    tool_context.state['sentiment_output'] = result\n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'reason': reason,\n",
    "        'sentiment': result.model_dump(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Define Test Utterances\n",
    "Craft a balanced set of positive, negative, mixed, and neutral utterances plus their expected tone, then store everything inside a pandas `DataFrame` for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_utterances = [\n",
    "    {\n",
    "        'label': 'gratitude_positive',\n",
    "        'tone_hint': 'High satisfaction, negligible frustration',\n",
    "        'text': 'Thank you for clarifying the booster timing—this finally makes sense.'\n",
    "    },\n",
    "    {\n",
    "        'label': 'anxious_negative',\n",
    "        'tone_hint': 'High frustration and confusion',\n",
    "        'text': 'I am terrified because every clinic gives me a different answer and I still have no appointment.'\n",
    "    },\n",
    "    {\n",
    "        'label': 'curious_neutral',\n",
    "        'tone_hint': 'Low across the board, fact-seeking',\n",
    "        'text': 'Could you remind me which vaccines are recommended for teachers this fall?'\n",
    "    },\n",
    "    {\n",
    "        'label': 'mixed_relief',\n",
    "        'tone_hint': 'Moderate satisfaction with residual confusion',\n",
    "        'text': 'I feel relieved after reading the brochure, but I am still unsure about the side effects timeline.'\n",
    "    },\n",
    "    {\n",
    "        'label': 'impatient_negative',\n",
    "        'tone_hint': 'Low satisfaction, high frustration',\n",
    "        'text': 'This waiting list is ridiculous and I am losing patience with the whole process.'\n",
    "    },\n",
    "    {\n",
    "        'label': 'optimistic_positive',\n",
    "        'tone_hint': 'High satisfaction, low frustration',\n",
    "        'text': 'Great news about the updated pediatric schedule—it gives me confidence to book right away.'\n",
    "    },\n",
    "]\n",
    "\n",
    "utterance_df = pd.DataFrame(test_utterances)\n",
    "utterance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Run Sentiment Inference\n",
    "Invoke the autonomous sentiment tool for every utterance, capture latency, tool metadata, and the fused intensity outputs so we can compare the results side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results: list[dict[str, Any]] = []\n",
    "tool_errors: list[dict[str, Any]] = []\n",
    "\n",
    "for row in utterance_df.itertuples(index=False):\n",
    "    ctx = NotebookToolContext(state={'user:input': row.text})\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        response = run_sentiment_tool(ctx)\n",
    "        elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "        output: SentimentOutput = ctx.state['sentiment_output']\n",
    "        results.append(\n",
    "            {\n",
    "                'label': row.label,\n",
    "                'tone_hint': row.tone_hint,\n",
    "                'text': row.text,\n",
    "                'reason': response['reason'],\n",
    "                'satisfaction': output.satisfaction,\n",
    "                'frustration': output.frustration,\n",
    "                'confusion': output.confusion,\n",
    "                'latency_ms': round(elapsed_ms, 1),\n",
    "            }\n",
    "        )\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "        tool_errors.append(\n",
    "            {\n",
    "                'label': row.label,\n",
    "                'error': str(exc),\n",
    "                'latency_ms': round(elapsed_ms, 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Validate Outputs and Diagnostics\n",
    "Ensure every invocation produced sentiment labels, report latency stats, surface any tool errors, and visualize the distribution of intensity levels across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if results_df.empty:\n",
    "    raise AssertionError('No successful sentiment runs were recorded.')\n",
    "\n",
    "for column in ['satisfaction', 'frustration', 'confusion']:\n",
    "    if results_df[column].isna().any():\n",
    "        raise AssertionError(f\"Column {column} contains missing values.\")\n",
    "\n",
    "latency_stats = {\n",
    "    'min_ms': results_df['latency_ms'].min(),\n",
    "    'max_ms': results_df['latency_ms'].max(),\n",
    "    'avg_ms': round(results_df['latency_ms'].mean(), 1),\n",
    "}\n",
    "print('Latency summary (ms):', latency_stats)\n",
    "\n",
    "if tool_errors:\n",
    "    print('Tool errors detected:')\n",
    "    display(pd.DataFrame(tool_errors))\n",
    "else:\n",
    "    print('All tool invocations completed without exceptions.')\n",
    "\n",
    "tall_df = (\n",
    "    results_df\n",
    "    .melt(id_vars=['label'], value_vars=['satisfaction', 'frustration', 'confusion'],\n",
    "          var_name='emotion', value_name='intensity')\n",
    ")\n",
    "counts = tall_df.value_counts(['emotion', 'intensity']).unstack(fill_value=0)\n",
    "print('\\nIntensity counts by emotion:')\n",
    "display(counts)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "counts.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sentiment Tool Intensity Distribution')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Helper: Probe Custom Utterances\n",
    "Use this utility to feed any arbitrary text through the same autonomous tool flow without editing earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_sentiment(text: str) -> dict[str, Any]:\n",
    "    ctx = NotebookToolContext(state={'user:input': text})\n",
    "    response = run_sentiment_tool(ctx)\n",
    "    output: SentimentOutput = ctx.state['sentiment_output']\n",
    "    return {\n",
    "        'input': text,\n",
    "        'status': response['status'],\n",
    "        'reason': response['reason'],\n",
    "        'satisfaction': output.satisfaction,\n",
    "        'frustration': output.frustration,\n",
    "        'confusion': output.confusion,\n",
    "    }\n",
    "\n",
    "probe_sentiment('I appreciate how clearly you explained the booster requirements.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaxtalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
